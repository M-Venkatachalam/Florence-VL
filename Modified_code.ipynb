{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Florence-VL Project: Replicating Results and Fine-Tuning\n",
        "## Introduction\n",
        "This notebook replicates and fine-tunes the Florence-VL model on the MS-COCO dataset for vision-language tasks. It includes:\n",
        "1. Dataset preparation (MS-COCO).\n",
        "2. Loading pretrained weights for Florence-VL.\n",
        "3. Fine-tuning the model.\n",
        "4. Evaluating results and discussing observations.\n"
      ],
      "metadata": {
        "id": "KwdlGNnDl3Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "eQxbBp5smAEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!git clone https://github.com/JiuhaiChen/Florence-VL.git\n",
        "!pip install torch torchvision transformers tqdm matplotlib"
      ],
      "metadata": {
        "id": "XJKbGNXkl88d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "upYKntK4mOXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p datasets/coco/{train2017,val2017,annotations}\n",
        "\n",
        "# Download and extract training images\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip -P datasets/coco/\n",
        "!unzip datasets/coco/train2017.zip -d datasets/coco/\n",
        "\n",
        "# Download and extract validation images\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip -P datasets/coco/\n",
        "!unzip datasets/coco/val2017.zip -d datasets/coco/\n",
        "\n",
        "# Download and extract annotations\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P datasets/coco/\n",
        "!unzip datasets/coco/annotations_trainval2017.zip -d datasets/coco/annotations/\n"
      ],
      "metadata": {
        "id": "CYlVgGX7maPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define paths\n",
        "COCO_TRAIN_IMAGES = \"./datasets/coco/train2017/\"\n",
        "COCO_VAL_IMAGES = \"./datasets/coco/val2017/\"\n",
        "COCO_ANNOTATIONS_TRAIN = \"./datasets/coco/annotations/captions_train2017.json\"\n",
        "COCO_ANNOTATIONS_VAL = \"./datasets/coco/annotations/captions_val2017.json\"\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load MS-COCO dataset\n",
        "train_dataset = datasets.CocoCaptions(root=COCO_TRAIN_IMAGES, annFile=COCO_ANNOTATIONS_TRAIN, transform=transform)\n",
        "val_dataset = datasets.CocoCaptions(root=COCO_VAL_IMAGES, annFile=COCO_ANNOTATIONS_VAL, transform=transform)\n",
        "\n",
        "# Display dataset stats\n",
        "print(f\"Training Samples: {len(train_dataset)}\")\n",
        "print(f\"Validation Samples: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "EDedqxwkmLpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pre-trained Model"
      ],
      "metadata": {
        "id": "Ohi7w05ImSvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Define paths\n",
        "CKPT_PATH = \"./checkpoints/florence_vl_weights.pth\"\n",
        "VIT_PATH = \"./checkpoints/vision_tower\"\n",
        "\n",
        "# Load Florence-VL model (placeholder using CLIP for illustration)\n",
        "class FlorenceVLModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlorenceVLModel, self).__init__()\n",
        "        self.encoder = torch.nn.Linear(2048, 512)  # Example encoder\n",
        "        self.decoder = torch.nn.Linear(512, 2048)  # Example decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Initialize model\n",
        "model = FlorenceVLModel()\n",
        "\n",
        "# Load pretrained weights\n",
        "if os.path.exists(CKPT_PATH):\n",
        "    state_dict = torch.load(CKPT_PATH)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Pretrained weights loaded successfully.\")\n",
        "else:\n",
        "    print(\"Pretrained weights not found.\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "ra7r9vdrmRna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tune the Model"
      ],
      "metadata": {
        "id": "zLhTkqodoimJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Adjust loss to fit Florence output\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3  # Set to 3 for quick experiments\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions in tqdm(train_loader):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, captions)  # Adjust loss computation\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "KQw3P1-7of_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "9vUhFbR1ooRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions in val_loader:\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Placeholder for metric calculation (e.g., Recall@1)\n",
        "        correct = (outputs.argmax(dim=1) == captions).sum().item()\n",
        "        total_correct += correct\n",
        "        total_samples += captions.size(0)\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "CxKVUFVxolz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "GifowzHYos2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "output_path = \"./checkpoints/florence_vl_finetuned.pth\"\n",
        "torch.save(model.state_dict(), output_path)\n",
        "print(f\"Fine-tuned model saved to {'./checkpoints/save_models/'}.\")\n"
      ],
      "metadata": {
        "id": "5BUXcJrJoq8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}